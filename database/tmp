---------------------------------------------------------------------------
PicklingError                             Traceback (most recent call last)
<ipython-input-1-f69c1bae95ce> in <module>()
    937 # change df to have key, value with min_date at the end
    938 mtime_0 = time_rdd.keyBy(lambda x: x[0])
--> 939 mtime_1 = mtime_0.reduceByKey(lambda x, y: x if x[2] < y[2] else y)
    940 # (key, min_date)
    941 mtime_2 = mtime_1.mapValues(lambda x: x[2])

/usr/local/spark/python/pyspark/rdd.pyc in reduceByKey(self, func, numPartitions, partitionFunc)
   1606         [('a', 2), ('b', 1)]
   1607         """
-> 1608         return self.combineByKey(lambda x: x, func, func, numPartitions, partitionFunc)
   1609 
   1610     def reduceByKeyLocally(self, func):

/usr/local/spark/python/pyspark/rdd.pyc in combineByKey(self, createCombiner, mergeValue, mergeCombiners, numPartitions, partitionFunc)
   1844 
   1845         locally_combined = self.mapPartitions(combineLocally, preservesPartitioning=True)
-> 1846         shuffled = locally_combined.partitionBy(numPartitions, partitionFunc)
   1847 
   1848         def _mergeCombiners(iterator):

/usr/local/spark/python/pyspark/rdd.pyc in partitionBy(self, numPartitions, partitionFunc)
   1781         with SCCallSiteSync(self.context) as css:
   1782             pairRDD = self.ctx._jvm.PairwiseRDD(
-> 1783                 keyed._jrdd.rdd()).asJavaPairRDD()
   1784             jpartitioner = self.ctx._jvm.PythonPartitioner(numPartitions,
   1785                                                            id(partitionFunc))

/usr/local/spark/python/pyspark/rdd.pyc in _jrdd(self)
   2453 
   2454         wrapped_func = _wrap_function(self.ctx, self.func, self._prev_jrdd_deserializer,
-> 2455                                       self._jrdd_deserializer, profiler)
   2456         python_rdd = self.ctx._jvm.PythonRDD(self._prev_jrdd.rdd(), wrapped_func,
   2457                                              self.preservesPartitioning)

/usr/local/spark/python/pyspark/rdd.pyc in _wrap_function(sc, func, deserializer, serializer, profiler)
   2386     assert serializer, "serializer should not be empty"
   2387     command = (func, profiler, deserializer, serializer)
-> 2388     pickled_command, broadcast_vars, env, includes = _prepare_for_python_RDD(sc, command)
   2389     return sc._jvm.PythonFunction(bytearray(pickled_command), env, includes, sc.pythonExec,
   2390                                   sc.pythonVer, broadcast_vars, sc._javaAccumulator)

/usr/local/spark/python/pyspark/rdd.pyc in _prepare_for_python_RDD(sc, command)
   2372     # the serialized command will be compressed by broadcast
   2373     ser = CloudPickleSerializer()
-> 2374     pickled_command = ser.dumps(command)
   2375     if len(pickled_command) > (1 << 20):  # 1M
   2376         # The broadcast will have same life cycle as created PythonRDD

/usr/local/spark/python/pyspark/serializers.pyc in dumps(self, obj)
    462 
    463     def dumps(self, obj):
--> 464         return cloudpickle.dumps(obj, 2)
    465 
    466 

/usr/local/spark/python/pyspark/cloudpickle.pyc in dumps(obj, protocol)
    702 
    703     cp = CloudPickler(file,protocol)
--> 704     cp.dump(obj)
    705 
    706     return file.getvalue()

/usr/local/spark/python/pyspark/cloudpickle.pyc in dump(self, obj)
    146         self.inject_addons()
    147         try:
--> 148             return Pickler.dump(self, obj)
    149         except RuntimeError as e:
    150             if 'recursion' in e.args[0]:

/usr/lib/python2.7/pickle.pyc in dump(self, obj)
    222         if self.proto >= 2:
    223             self.write(PROTO + chr(self.proto))
--> 224         self.save(obj)
    225         self.write(STOP)
    226 

/usr/lib/python2.7/pickle.pyc in save(self, obj)
    284         f = self.dispatch.get(t)
    285         if f:
--> 286             f(self, obj) # Call unbound method with explicit self
    287             return
    288 

/usr/lib/python2.7/pickle.pyc in save_tuple(self, obj)
    566         write(MARK)
    567         for element in obj:
--> 568             save(element)
    569 
    570         if id(obj) in memo:

/usr/lib/python2.7/pickle.pyc in save(self, obj)
    284         f = self.dispatch.get(t)
    285         if f:
--> 286             f(self, obj) # Call unbound method with explicit self
    287             return
    288 

/usr/local/spark/python/pyspark/cloudpickle.pyc in save_function(self, obj, name)
    253             klass = getattr(themodule, name, None)
    254             if klass is None or klass is not obj:
--> 255                 self.save_function_tuple(obj)
    256                 return
    257 

/usr/local/spark/python/pyspark/cloudpickle.pyc in save_function_tuple(self, func)
    290         # create a skeleton function object and memoize it
    291         save(_make_skel_func)
--> 292         save((code, closure, base_globals))
    293         write(pickle.REDUCE)
    294         self.memoize(func)

/usr/lib/python2.7/pickle.pyc in save(self, obj)
    284         f = self.dispatch.get(t)
    285         if f:
--> 286             f(self, obj) # Call unbound method with explicit self
    287             return
    288 

/usr/lib/python2.7/pickle.pyc in save_tuple(self, obj)
    552         if n <= 3 and proto >= 2:
    553             for element in obj:
--> 554                 save(element)
    555             # Subtle.  Same as in the big comment below.
    556             if id(obj) in memo:

/usr/lib/python2.7/pickle.pyc in save(self, obj)
    284         f = self.dispatch.get(t)
    285         if f:
--> 286             f(self, obj) # Call unbound method with explicit self
    287             return
    288 

/usr/lib/python2.7/pickle.pyc in save_list(self, obj)
    604 
    605         self.memoize(obj)
--> 606         self._batch_appends(iter(obj))
    607 
    608     dispatch[ListType] = save_list

/usr/lib/python2.7/pickle.pyc in _batch_appends(self, items)
    637                 write(MARK)
    638                 for x in tmp:
--> 639                     save(x)
    640                 write(APPENDS)
    641             elif n:

/usr/lib/python2.7/pickle.pyc in save(self, obj)
    284         f = self.dispatch.get(t)
    285         if f:
--> 286             f(self, obj) # Call unbound method with explicit self
    287             return
    288 

/usr/local/spark/python/pyspark/cloudpickle.pyc in save_function(self, obj, name)
    253             klass = getattr(themodule, name, None)
    254             if klass is None or klass is not obj:
--> 255                 self.save_function_tuple(obj)
    256                 return
    257 

/usr/local/spark/python/pyspark/cloudpickle.pyc in save_function_tuple(self, func)
    290         # create a skeleton function object and memoize it
    291         save(_make_skel_func)
--> 292         save((code, closure, base_globals))
    293         write(pickle.REDUCE)
    294         self.memoize(func)

/usr/lib/python2.7/pickle.pyc in save(self, obj)
    284         f = self.dispatch.get(t)
    285         if f:
--> 286             f(self, obj) # Call unbound method with explicit self
    287             return
    288 

/usr/lib/python2.7/pickle.pyc in save_tuple(self, obj)
    552         if n <= 3 and proto >= 2:
    553             for element in obj:
--> 554                 save(element)
    555             # Subtle.  Same as in the big comment below.
    556             if id(obj) in memo:

/usr/lib/python2.7/pickle.pyc in save(self, obj)
    284         f = self.dispatch.get(t)
    285         if f:
--> 286             f(self, obj) # Call unbound method with explicit self
    287             return
    288 

/usr/lib/python2.7/pickle.pyc in save_list(self, obj)
    604 
    605         self.memoize(obj)
--> 606         self._batch_appends(iter(obj))
    607 
    608     dispatch[ListType] = save_list

/usr/lib/python2.7/pickle.pyc in _batch_appends(self, items)
    637                 write(MARK)
    638                 for x in tmp:
--> 639                     save(x)
    640                 write(APPENDS)
    641             elif n:

/usr/lib/python2.7/pickle.pyc in save(self, obj)
    284         f = self.dispatch.get(t)
    285         if f:
--> 286             f(self, obj) # Call unbound method with explicit self
    287             return
    288 

/usr/local/spark/python/pyspark/cloudpickle.pyc in save_function(self, obj, name)
    253             klass = getattr(themodule, name, None)
    254             if klass is None or klass is not obj:
--> 255                 self.save_function_tuple(obj)
    256                 return
    257 

/usr/local/spark/python/pyspark/cloudpickle.pyc in save_function_tuple(self, func)
    290         # create a skeleton function object and memoize it
    291         save(_make_skel_func)
--> 292         save((code, closure, base_globals))
    293         write(pickle.REDUCE)
    294         self.memoize(func)

/usr/lib/python2.7/pickle.pyc in save(self, obj)
    284         f = self.dispatch.get(t)
    285         if f:
--> 286             f(self, obj) # Call unbound method with explicit self
    287             return
    288 

/usr/lib/python2.7/pickle.pyc in save_tuple(self, obj)
    552         if n <= 3 and proto >= 2:
    553             for element in obj:
--> 554                 save(element)
    555             # Subtle.  Same as in the big comment below.
    556             if id(obj) in memo:

/usr/lib/python2.7/pickle.pyc in save(self, obj)
    284         f = self.dispatch.get(t)
    285         if f:
--> 286             f(self, obj) # Call unbound method with explicit self
    287             return
    288 

/usr/lib/python2.7/pickle.pyc in save_list(self, obj)
    604 
    605         self.memoize(obj)
--> 606         self._batch_appends(iter(obj))
    607 
    608     dispatch[ListType] = save_list

/usr/lib/python2.7/pickle.pyc in _batch_appends(self, items)
    637                 write(MARK)
    638                 for x in tmp:
--> 639                     save(x)
    640                 write(APPENDS)
    641             elif n:

/usr/lib/python2.7/pickle.pyc in save(self, obj)
    284         f = self.dispatch.get(t)
    285         if f:
--> 286             f(self, obj) # Call unbound method with explicit self
    287             return
    288 

/usr/local/spark/python/pyspark/cloudpickle.pyc in save_function(self, obj, name)
    253             klass = getattr(themodule, name, None)
    254             if klass is None or klass is not obj:
--> 255                 self.save_function_tuple(obj)
    256                 return
    257 

/usr/local/spark/python/pyspark/cloudpickle.pyc in save_function_tuple(self, func)
    290         # create a skeleton function object and memoize it
    291         save(_make_skel_func)
--> 292         save((code, closure, base_globals))
    293         write(pickle.REDUCE)
    294         self.memoize(func)

/usr/lib/python2.7/pickle.pyc in save(self, obj)
    284         f = self.dispatch.get(t)
    285         if f:
--> 286             f(self, obj) # Call unbound method with explicit self
    287             return
    288 

/usr/lib/python2.7/pickle.pyc in save_tuple(self, obj)
    552         if n <= 3 and proto >= 2:
    553             for element in obj:
--> 554                 save(element)
    555             # Subtle.  Same as in the big comment below.
    556             if id(obj) in memo:

/usr/lib/python2.7/pickle.pyc in save(self, obj)
    284         f = self.dispatch.get(t)
    285         if f:
--> 286             f(self, obj) # Call unbound method with explicit self
    287             return
    288 

/usr/lib/python2.7/pickle.pyc in save_list(self, obj)
    604 
    605         self.memoize(obj)
--> 606         self._batch_appends(iter(obj))
    607 
    608     dispatch[ListType] = save_list

/usr/lib/python2.7/pickle.pyc in _batch_appends(self, items)
    637                 write(MARK)
    638                 for x in tmp:
--> 639                     save(x)
    640                 write(APPENDS)
    641             elif n:

/usr/lib/python2.7/pickle.pyc in save(self, obj)
    284         f = self.dispatch.get(t)
    285         if f:
--> 286             f(self, obj) # Call unbound method with explicit self
    287             return
    288 

/usr/local/spark/python/pyspark/cloudpickle.pyc in save_function(self, obj, name)
    253             klass = getattr(themodule, name, None)
    254             if klass is None or klass is not obj:
--> 255                 self.save_function_tuple(obj)
    256                 return
    257 

/usr/local/spark/python/pyspark/cloudpickle.pyc in save_function_tuple(self, func)
    290         # create a skeleton function object and memoize it
    291         save(_make_skel_func)
--> 292         save((code, closure, base_globals))
    293         write(pickle.REDUCE)
    294         self.memoize(func)

/usr/lib/python2.7/pickle.pyc in save(self, obj)
    284         f = self.dispatch.get(t)
    285         if f:
--> 286             f(self, obj) # Call unbound method with explicit self
    287             return
    288 

/usr/lib/python2.7/pickle.pyc in save_tuple(self, obj)
    552         if n <= 3 and proto >= 2:
    553             for element in obj:
--> 554                 save(element)
    555             # Subtle.  Same as in the big comment below.
    556             if id(obj) in memo:

/usr/lib/python2.7/pickle.pyc in save(self, obj)
    284         f = self.dispatch.get(t)
    285         if f:
--> 286             f(self, obj) # Call unbound method with explicit self
    287             return
    288 

/usr/lib/python2.7/pickle.pyc in save_list(self, obj)
    604 
    605         self.memoize(obj)
--> 606         self._batch_appends(iter(obj))
    607 
    608     dispatch[ListType] = save_list

/usr/lib/python2.7/pickle.pyc in _batch_appends(self, items)
    640                 write(APPENDS)
    641             elif n:
--> 642                 save(tmp[0])
    643                 write(APPEND)
    644             # else tmp is empty, and we're done

/usr/lib/python2.7/pickle.pyc in save(self, obj)
    284         f = self.dispatch.get(t)
    285         if f:
--> 286             f(self, obj) # Call unbound method with explicit self
    287             return
    288 

/usr/local/spark/python/pyspark/cloudpickle.pyc in save_function(self, obj, name)
    247         if islambda(obj) or obj.__code__.co_filename == '<stdin>' or themodule is None:
    248             #print("save global", islambda(obj), obj.__code__.co_filename, modname, themodule)
--> 249             self.save_function_tuple(obj)
    250             return
    251         else:

/usr/local/spark/python/pyspark/cloudpickle.pyc in save_function_tuple(self, func)
    295 
    296         # save the rest of the func data needed by _fill_function
--> 297         save(f_globals)
    298         save(defaults)
    299         save(dct)

/usr/lib/python2.7/pickle.pyc in save(self, obj)
    284         f = self.dispatch.get(t)
    285         if f:
--> 286             f(self, obj) # Call unbound method with explicit self
    287             return
    288 

/usr/lib/python2.7/pickle.pyc in save_dict(self, obj)
    653 
    654         self.memoize(obj)
--> 655         self._batch_setitems(obj.iteritems())
    656 
    657     dispatch[DictionaryType] = save_dict

/usr/lib/python2.7/pickle.pyc in _batch_setitems(self, items)
    685                 for k, v in tmp:
    686                     save(k)
--> 687                     save(v)
    688                 write(SETITEMS)
    689             elif n:

/usr/lib/python2.7/pickle.pyc in save(self, obj)
    284         f = self.dispatch.get(t)
    285         if f:
--> 286             f(self, obj) # Call unbound method with explicit self
    287             return
    288 

/usr/local/spark/python/pyspark/cloudpickle.pyc in save_function(self, obj, name)
    247         if islambda(obj) or obj.__code__.co_filename == '<stdin>' or themodule is None:
    248             #print("save global", islambda(obj), obj.__code__.co_filename, modname, themodule)
--> 249             self.save_function_tuple(obj)
    250             return
    251         else:

/usr/local/spark/python/pyspark/cloudpickle.pyc in save_function_tuple(self, func)
    295 
    296         # save the rest of the func data needed by _fill_function
--> 297         save(f_globals)
    298         save(defaults)
    299         save(dct)

/usr/lib/python2.7/pickle.pyc in save(self, obj)
    284         f = self.dispatch.get(t)
    285         if f:
--> 286             f(self, obj) # Call unbound method with explicit self
    287             return
    288 

/usr/lib/python2.7/pickle.pyc in save_dict(self, obj)
    653 
    654         self.memoize(obj)
--> 655         self._batch_setitems(obj.iteritems())
    656 
    657     dispatch[DictionaryType] = save_dict

/usr/lib/python2.7/pickle.pyc in _batch_setitems(self, items)
    685                 for k, v in tmp:
    686                     save(k)
--> 687                     save(v)
    688                 write(SETITEMS)
    689             elif n:

/usr/lib/python2.7/pickle.pyc in save(self, obj)
    329 
    330         # Save the reduce() output and finally memoize the object
--> 331         self.save_reduce(obj=obj, *rv)
    332 
    333     def persistent_id(self, obj):

/usr/local/spark/python/pyspark/cloudpickle.pyc in save_reduce(self, func, args, state, listitems, dictitems, obj)
    563             if obj is not None and cls is not obj.__class__:
    564                 raise pickle.PicklingError(
--> 565                     "args[0] from __newobj__ args has the wrong class")
    566             args = args[1:]
    567             save(cls)

PicklingError: args[0] from __newobj__ args has the wrong class
